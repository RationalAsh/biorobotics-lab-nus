---
# {
#     "copyright": "arXiv.org perpetual,  non-exclusive license",
#     "year": "2020",
#     "publisher": "arXiv",
#     "title": "Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with Advantage Weighted Mixture Policy(SAC-AWMP)",
#     "keywords": "Machine Learning (cs.LG),  Artificial Intelligence (cs.AI),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences",
#     "author": "Hou,  Zhimin and Zhang,  Kuangen and Wan,  Yi and Li,  Dongyu and Fu,  Chenglong and Yu,  Haoyong",
#     "url": "https://arxiv.org/abs/2002.02829",
#     "doi": "10.48550/ARXIV.2002.02829",
#     "ENTRYTYPE": "misc",
#     "ID": "https://doi.org/10.48550/arxiv.2002.02829"
# }abstract: ''
authors:
- Hou,  Zhimin
- Zhang,  Kuangen
- Wan,  Yi
- Li,  Dongyu
- Fu,  Chenglong
- Yu,  Haoyong
categories: []
date: '2020-01-01'
doi: 10.48550/ARXIV.2002.02829
featured: false
image:
    caption: ''
    focal_point: ''
    preview_only: false
publication: '*arXiv*'
publication_types:
- '3'
publishDate: '2020-01-01T00:00:00+08:00'
slides: ''
tags: []
title: 'Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with
    Advantage Weighted Mixture Policy(SAC-AWMP)'
url_code: ''
url_dataset: ''
url_pdf: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: https://arxiv.org/abs/2002.02829
url_video: ''
---
